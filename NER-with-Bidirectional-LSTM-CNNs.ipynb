{"cells":[{"cell_type":"markdown","metadata":{"id":"5aZ3C3so07p-"},"source":["#Prepro"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1667866890849,"user":{"displayName":"Phạm Minh Tài","userId":"17160610525266134724"},"user_tz":-420},"id":"Cga8bP161BAi"},"outputs":[],"source":["import numpy as np\n","from keras_preprocessing.sequence import pad_sequences\n","\n","def readfile(filename):\n","    '''\n","    read file\n","    return format :\n","    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n","    '''\n","    f = open(filename)\n","    sentences = []\n","    sentence = []\n","    for line in f:\n","        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n","            if len(sentence) > 0:\n","                sentences.append(sentence)\n","                sentence = []\n","            continue\n","        splits = line.split(' ')\n","        sentence.append([splits[0],splits[-1]])\n","\n","    if len(sentence) >0:\n","        sentences.append(sentence)\n","        sentence = []\n","    return sentences\n","\n","def getCasing(word, caseLookup):   \n","    casing = 'other'\n","    \n","    numDigits = 0\n","    for char in word:\n","        if char.isdigit():\n","            numDigits += 1\n","            \n","    digitFraction = numDigits / float(len(word))\n","    \n","    if word.isdigit(): #Is a digit\n","        casing = 'numeric'\n","    elif digitFraction > 0.5:\n","        casing = 'mainly_numeric'\n","    elif word.islower(): #All lower case\n","        casing = 'allLower'\n","    elif word.isupper(): #All upper case\n","        casing = 'allUpper'\n","    elif word[0].isupper(): #is a title, initial char upper, then all lower\n","        casing = 'initialUpper'\n","    elif numDigits > 0:\n","        casing = 'contains_digit'\n","    return caseLookup[casing]\n","    \n","def createBatches(data):\n","    l = []\n","    for i in data:\n","        l.append(len(i[0]))\n","    l = set(l)\n","    batches = []\n","    batch_len = []\n","    z = 0\n","    for i in l:\n","        for batch in data:\n","            if len(batch[0]) == i:\n","                batches.append(batch)\n","                z += 1\n","        batch_len.append(z)\n","    return batches,batch_len\n","\n","def createBatches(data):\n","    l = []\n","    for i in data:\n","        l.append(len(i[0]))\n","    l = set(l)\n","    batches = []\n","    batch_len = []\n","    z = 0\n","    for i in l:\n","        for batch in data:\n","            if len(batch[0]) == i:\n","                batches.append(batch)\n","                z += 1\n","        batch_len.append(z)\n","    return batches,batch_len\n","\n","def createMatrices(sentences, word2Idx, label2Idx, case2Idx,char2Idx):\n","    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n","    paddingIdx = word2Idx['PADDING_TOKEN']    \n","        \n","    dataset = []\n","    \n","    wordCount = 0\n","    unknownWordCount = 0\n","    \n","    for sentence in sentences:\n","        wordIndices = []    \n","        caseIndices = []\n","        charIndices = []\n","        labelIndices = []\n","        \n","        for word,char,label in sentence:  \n","            wordCount += 1\n","            if word in word2Idx:\n","                wordIdx = word2Idx[word]\n","            elif word.lower() in word2Idx:\n","                wordIdx = word2Idx[word.lower()]                 \n","            else:\n","                wordIdx = unknownIdx\n","                unknownWordCount += 1\n","            charIdx = []\n","            for x in char:\n","                charIdx.append(char2Idx[x])\n","            #Get the label and map to int            \n","            wordIndices.append(wordIdx)\n","            caseIndices.append(getCasing(word, case2Idx))\n","            charIndices.append(charIdx)\n","            labelIndices.append(label2Idx[label])\n","           \n","        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n","        \n","    return dataset\n","\n","def iterate_minibatches(dataset,batch_len): \n","    start = 0\n","    for i in batch_len:\n","        tokens = []\n","        caseing = []\n","        char = []\n","        labels = []\n","        data = dataset[start:i]\n","        start = i\n","        for dt in data:\n","            t,c,ch,l = dt\n","            l = np.expand_dims(l,-1)\n","            tokens.append(t)\n","            caseing.append(c)\n","            char.append(ch)\n","            labels.append(l)\n","        yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)\n","\n","def addCharInformatioin(Sentences):\n","    for i,sentence in enumerate(Sentences):\n","        for j,data in enumerate(sentence):\n","            chars = [c for c in data[0]]\n","            Sentences[i][j] = [data[0],chars,data[1]]\n","    return Sentences\n","\n","def padding(Sentences):\n","    maxlen = 52\n","    for sentence in Sentences:\n","        char = sentence[2]\n","        for x in char:\n","            maxlen = max(maxlen,len(x))\n","    for i,sentence in enumerate(Sentences):\n","        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n","    return Sentences"]},{"cell_type":"markdown","metadata":{"id":"AWWgPZ3k1GMC"},"source":["#Validation"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":290,"status":"ok","timestamp":1667866897611,"user":{"displayName":"Phạm Minh Tài","userId":"17160610525266134724"},"user_tz":-420},"id":"pBzVaQpX1Ia6"},"outputs":[],"source":["import numpy as np\n","\n","\n","#Method to compute the accruarcy. Call predict_labels to get the labels for the dataset\n","def compute_f1(predictions, correct, idx2Label): \n","    label_pred = []    \n","    for sentence in predictions:\n","        label_pred.append([idx2Label[element] for element in sentence])\n","        \n","    label_correct = []    \n","    for sentence in correct:\n","        label_correct.append([idx2Label[element] for element in sentence])\n","    \n","    prec = compute_precision(label_pred, label_correct)\n","    rec = compute_precision(label_correct, label_pred)\n","    \n","    f1 = 0\n","    if (rec+prec) > 0:\n","        f1 = 2.0 * prec * rec / (prec + rec);\n","        \n","    return prec, rec, f1\n","\n","def compute_precision(guessed_sentences, correct_sentences):\n","    assert(len(guessed_sentences) == len(correct_sentences))\n","    correctCount = 0\n","    count = 0\n","    \n","    for sentenceIdx in range(len(guessed_sentences)):\n","        guessed = guessed_sentences[sentenceIdx]\n","        correct = correct_sentences[sentenceIdx]\n","        assert(len(guessed) == len(correct))\n","        idx = 0\n","        while idx < len(guessed):\n","            if guessed[idx][0] == 'B': #A new chunk starts\n","                count += 1\n","                \n","                if guessed[idx] == correct[idx]:\n","                    idx += 1\n","                    correctlyFound = True\n","                    \n","                    while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n","                        if guessed[idx] != correct[idx]:\n","                            correctlyFound = False\n","                        \n","                        idx += 1\n","                    \n","                    if idx < len(guessed):\n","                        if correct[idx][0] == 'I': #The chunk in correct was longer\n","                            correctlyFound = False\n","                        \n","                    \n","                    if correctlyFound:\n","                        correctCount += 1\n","                else:\n","                    idx += 1\n","            else:  \n","                idx += 1\n","    \n","    precision = 0\n","    if count > 0:    \n","        precision = float(correctCount) / count\n","        \n","    return precision"]},{"cell_type":"markdown","metadata":{"id":"KGzblmJ11NAP"},"source":["#NN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvlEgDrU1LY8","outputId":"8d098bf0-734f-41e0-c13b-f12ff809bb0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," char_input (InputLayer)        [(None, None, 52)]   0           []                               \n","                                                                                                  \n"," char_embedding (TimeDistribute  (None, None, 52, 30  2850       ['char_input[0][0]']             \n"," d)                             )                                                                 \n","                                                                                                  \n"," dropout (Dropout)              (None, None, 52, 30  0           ['char_embedding[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," time_distributed (TimeDistribu  (None, None, 52, 30  2730       ['dropout[0][0]']                \n"," ted)                           )                                                                 \n","                                                                                                  \n"," time_distributed_1 (TimeDistri  (None, None, 1, 30)  0          ['time_distributed[0][0]']       \n"," buted)                                                                                           \n","                                                                                                  \n"," words_input (InputLayer)       [(None, None)]       0           []                               \n","                                                                                                  \n"," casing_input (InputLayer)      [(None, None)]       0           []                               \n","                                                                                                  \n"," time_distributed_2 (TimeDistri  (None, None, 30)    0           ['time_distributed_1[0][0]']     \n"," buted)                                                                                           \n","                                                                                                  \n"," embedding (Embedding)          (None, None, 300)    6884700     ['words_input[0][0]']            \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 8)      64          ['casing_input[0][0]']           \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, None, 30)     0           ['time_distributed_2[0][0]']     \n","                                                                                                  \n"," concatenate (Concatenate)      (None, None, 338)    0           ['embedding[0][0]',              \n","                                                                  'embedding_1[0][0]',            \n","                                                                  'dropout_1[0][0]']              \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, None, 400)    862400      ['concatenate[0][0]']            \n","                                                                                                  \n"," time_distributed_3 (TimeDistri  (None, None, 9)     3609        ['bidirectional[0][0]']          \n"," buted)                                                                                           \n","                                                                                                  \n","==================================================================================================\n","Total params: 7,756,353\n","Trainable params: 871,589\n","Non-trainable params: 6,884,764\n","__________________________________________________________________________________________________\n","Epoch 0/50\n","64/64 [==============================] - 103s 1s/step\n"," \n","Epoch 1/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 2/50\n","64/64 [==============================] - 117s 2s/step\n"," \n","Epoch 3/50\n","64/64 [==============================] - 84s 1s/step\n"," \n","Epoch 4/50\n","64/64 [==============================] - 85s 1s/step\n"," \n","Epoch 5/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 6/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 7/50\n","64/64 [==============================] - 84s 1s/step\n"," \n","Epoch 8/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 9/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 10/50\n","64/64 [==============================] - 84s 1s/step\n"," \n","Epoch 11/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 12/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 13/50\n","64/64 [==============================] - 87s 1s/step\n"," \n","Epoch 14/50\n","64/64 [==============================] - 90s 1s/step\n"," \n","Epoch 15/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 16/50\n","64/64 [==============================] - 84s 1s/step\n"," \n","Epoch 17/50\n","64/64 [==============================] - 84s 1s/step\n"," \n","Epoch 18/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 19/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 20/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 21/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 22/50\n","64/64 [==============================] - 84s 1s/step\n"," \n","Epoch 23/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 24/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 25/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 26/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 27/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 28/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 29/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 30/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 31/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 32/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 33/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 34/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 35/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 36/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 37/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 38/50\n","64/64 [==============================] - 84s 1s/step\n"," \n","Epoch 39/50\n","64/64 [==============================] - 84s 1s/step\n"," \n","Epoch 40/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","Epoch 41/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 42/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 43/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 44/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 45/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 46/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 47/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 48/50\n","64/64 [==============================] - 82s 1s/step\n"," \n","Epoch 49/50\n","64/64 [==============================] - 83s 1s/step\n"," \n","3250/3250 [==============================] - 323s 99ms/step\n","Dev-Data: Prec: 0.924, Rec: 0.929, F1: 0.927\n","2628/3453 [=====================>........] - ETA: 1:12"]}],"source":["import numpy as np\n","from keras.models import Model\n","from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n","from keras.utils import Progbar\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.initializers import RandomUniform\n","\n","epochs = 50\n","\n","def tag_dataset(dataset):\n","    correctLabels = []\n","    predLabels = []\n","    b = Progbar(len(dataset))\n","    for i,data in enumerate(dataset):    \n","        tokens, casing,char, labels = data\n","        tokens = np.asarray([tokens])     \n","        casing = np.asarray([casing])\n","        char = np.asarray([char])\n","        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n","        pred = pred.argmax(axis=-1) #Predict the classes            \n","        correctLabels.append(labels)\n","        predLabels.append(pred)\n","        b.update(i)\n","    b.update(i+1)\n","    return predLabels, correctLabels\n","\n","\n","trainSentences = readfile(\"data/train.txt\")\n","devSentences = readfile(\"data/valid.txt\")\n","testSentences = readfile(\"data/test.txt\")\n","\n","trainSentences = addCharInformatioin(trainSentences)\n","devSentences = addCharInformatioin(devSentences)\n","testSentences = addCharInformatioin(testSentences)\n","\n","labelSet = set()\n","words = {}\n","\n","for dataset in [trainSentences, devSentences, testSentences]:\n","    for sentence in dataset:\n","        for token,char,label in sentence:\n","            labelSet.add(label)\n","            words[token.lower()] = True\n","\n","# :: Create a mapping for the labels ::\n","label2Idx = {}\n","for label in labelSet:\n","    label2Idx[label] = len(label2Idx)\n","\n","# :: Hard coded case lookup ::\n","case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n","caseEmbeddings = np.identity(len(case2Idx), dtype='float32')\n","\n","\n","# :: Read in word embeddings ::\n","word2Idx = {}\n","wordEmbeddings = []\n","\n","fEmbeddings = open(\"embeddings/glove.6B.300d.txt\", encoding=\"utf-8\")\n","\n","for line in fEmbeddings:\n","    split = line.strip().split(\" \")\n","    word = split[0]\n","    \n","    if len(word2Idx) == 0: #Add padding+unknown\n","        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n","        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n","        wordEmbeddings.append(vector)\n","        \n","        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n","        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n","        wordEmbeddings.append(vector)\n","\n","    if split[0].lower() in words:\n","        vector = np.array([float(num) for num in split[1:]])\n","        wordEmbeddings.append(vector)\n","        word2Idx[split[0]] = len(word2Idx)\n","        \n","wordEmbeddings = np.array(wordEmbeddings)\n","\n","char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n","for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n","    char2Idx[c] = len(char2Idx)\n","\n","train_set = padding(createMatrices(trainSentences,word2Idx,  label2Idx, case2Idx,char2Idx))\n","dev_set = padding(createMatrices(devSentences,word2Idx, label2Idx, case2Idx,char2Idx))\n","test_set = padding(createMatrices(testSentences, word2Idx, label2Idx, case2Idx,char2Idx))\n","\n","idx2Label = {v: k for k, v in label2Idx.items()}\n","np.save(\"models/idx2Label.npy\",idx2Label)\n","np.save(\"models/word2Idx.npy\",word2Idx)\n","\n","train_batch,train_batch_len = createBatches(train_set)\n","dev_batch,dev_batch_len = createBatches(dev_set)\n","test_batch,test_batch_len = createBatches(test_set)\n","\n","\n","words_input = Input(shape=(None,),dtype='int32',name='words_input')\n","words = Embedding(input_dim=wordEmbeddings.shape[0], \n","                  output_dim=wordEmbeddings.shape[1],  \n","                  weights=[wordEmbeddings], trainable=False)(words_input)\n","casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n","casing = Embedding(output_dim=caseEmbeddings.shape[1], \n","                   input_dim=caseEmbeddings.shape[0], \n","                   weights=[caseEmbeddings], trainable=False)(casing_input)\n","character_input=Input(shape=(None,52,),name='char_input')\n","embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,\n","                                         embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), \n","                                         name='char_embedding')(character_input)\n","dropout= Dropout(0.5)(embed_char_out)\n","conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, \n","                                   padding='same',activation='tanh', \n","                                   strides=1))(dropout)\n","maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n","char = TimeDistributed(Flatten())(maxpool_out)\n","char = Dropout(0.5)(char)\n","output = concatenate([words, casing,char])\n","output = Bidirectional(LSTM(200, return_sequences=True, \n","                            dropout=0.50, recurrent_dropout=0.25))(output)\n","output = TimeDistributed(Dense(len(label2Idx), \n","                               activation='softmax'))(output)\n","model = Model(inputs=[words_input, casing_input,\n","                      character_input], outputs=[output])\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n","model.summary()\n","# plot_model(model, to_file='model.png')\n","\n","\n","for epoch in range(epochs):    \n","    print(\"Epoch %d/%d\"%(epoch,epochs))\n","    a = Progbar(len(train_batch_len))\n","    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n","        labels, tokens, casing,char = batch       \n","        model.train_on_batch([tokens, casing,char], labels)\n","        a.update(i)\n","    a.update(i+1)\n","    print(' ')\n","\n","model.save(\"models/model.h5\")\n","\n","#   Performance on dev dataset        \n","predLabels, correctLabels = tag_dataset(dev_batch)        \n","pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, idx2Label)\n","print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))\n","    \n","#   Performance on test dataset       \n","predLabels, correctLabels = tag_dataset(test_batch)        \n","pre_test, rec_test, f1_test= compute_f1(predLabels, correctLabels, idx2Label)\n","print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_test, rec_test, f1_test))"]},{"cell_type":"markdown","metadata":{"id":"E-rteOuC03oE"},"source":["#NER"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4336,"status":"ok","timestamp":1667866911458,"user":{"displayName":"Phạm Minh Tài","userId":"17160610525266134724"},"user_tz":-420},"id":"26SJ4PL800qe"},"outputs":[],"source":["import numpy as np\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","from keras.models import load_model\n","from keras_preprocessing.sequence import pad_sequences\n","from nltk import word_tokenize\n","\n","class Parser:\n","\n","    def __init__(self):\n","        # ::Hard coded char lookup ::\n","        self.char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n","        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n","            self.char2Idx[c] = len(self.char2Idx)\n","        # :: Hard coded case lookup ::\n","        self.case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3,\n","                         'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n","\n","    def load_models(self, loc=None):\n","        if not loc:\n","            loc = os.path.join(os.path.expanduser('~'), '.ner_model')\n","        self.model = load_model(os.path.join(loc,\"model.h5\"))\n","        # loading word2Idx\n","        self.word2Idx = np.load(os.path.join(loc,\"word2Idx.npy\"), allow_pickle=True).item()\n","        # loading idx2Label\n","        self.idx2Label = np.load(os.path.join(loc,\"idx2Label.npy\"), allow_pickle=True).item()\n","\n","    def getCasing(self,word, caseLookup):   \n","        casing = 'other'\n","        \n","        numDigits = 0\n","        for char in word:\n","            if char.isdigit():\n","                numDigits += 1\n","                \n","        digitFraction = numDigits / float(len(word))\n","        \n","        if word.isdigit(): #Is a digit\n","            casing = 'numeric'\n","        elif digitFraction > 0.5:\n","            casing = 'mainly_numeric'\n","        elif word.islower(): #All lower case\n","            casing = 'allLower'\n","        elif word.isupper(): #All upper case\n","            casing = 'allUpper'\n","        elif word[0].isupper(): #is a title, initial char upper, then all lower\n","            casing = 'initialUpper'\n","        elif numDigits > 0:\n","            casing = 'contains_digit'  \n","        return caseLookup[casing]\n","\n","    def createTensor(self,sentence, word2Idx,case2Idx,char2Idx):\n","        unknownIdx = word2Idx['UNKNOWN_TOKEN']\n","    \n","        wordIndices = []    \n","        caseIndices = []\n","        charIndices = []\n","            \n","        for word,char in sentence:  \n","            word = str(word)\n","            if word in word2Idx:\n","                wordIdx = word2Idx[word]\n","            elif word.lower() in word2Idx:\n","                wordIdx = word2Idx[word.lower()]                 \n","            else:\n","                wordIdx = unknownIdx\n","            charIdx = []\n","            for x in char:\n","                if x in char2Idx.keys():\n","                    charIdx.append(char2Idx[x])\n","                else:\n","                    charIdx.append(char2Idx['UNKNOWN'])   \n","            wordIndices.append(wordIdx)\n","            caseIndices.append(self.getCasing(word, case2Idx))\n","            charIndices.append(charIdx)\n","            \n","        return [wordIndices, caseIndices, charIndices]\n","\n","    def addCharInformation(self, sentence):\n","        return [[word, list(str(word))] for word in sentence]\n","\n","    def padding(self,Sentence):\n","        Sentence[2] = pad_sequences(Sentence[2],52,padding='post')\n","        return Sentence\n","\n","    def predict(self,Sentence):\n","        Sentence = words =  word_tokenize(Sentence)\n","        Sentence = self.addCharInformation(Sentence)\n","        Sentence = self.padding(self.createTensor(Sentence,self.word2Idx,self.case2Idx,self.char2Idx))\n","        tokens, casing,char = Sentence\n","        tokens = np.asarray([tokens])     \n","        casing = np.asarray([casing])\n","        char = np.asarray([char])\n","        pred = self.model.predict([tokens, casing,char], verbose=False)[0]   \n","        pred = pred.argmax(axis=-1)\n","        pred = [self.idx2Label[x].strip() for x in pred]\n","        return list(zip(words,pred))"]},{"cell_type":"markdown","metadata":{"id":"NBnNz7zd19ZM"},"source":["# Test trained model"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2448,"status":"ok","timestamp":1667867339754,"user":{"displayName":"Phạm Minh Tài","userId":"17160610525266134724"},"user_tz":-420},"id":"F9pRpry9163o","outputId":"d329d03e-5475-487c-f811-c173e7ce3ff6"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to C:\\Users\\Minh\n","[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["[('Michel', 'B-PER'),\n"," ('Jordan', 'I-PER'),\n"," ('would', 'O'),\n"," ('choose', 'O'),\n"," ('Brush', 'B-ORG'),\n"," ('.', 'O'),\n"," ('Steve', 'B-PER'),\n"," ('went', 'O'),\n"," ('to', 'O'),\n"," ('United', 'B-LOC'),\n"," ('States', 'I-LOC'),\n"," ('of', 'I-LOC'),\n"," ('America', 'I-LOC')]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')\n","\n","p = Parser()\n","\n","p.load_models(\"models\")\n","\n","#p.predict(\"Steve went to Paris\")\n","p.predict(\"Michel Jordan would choose Brush. Steve went to United States of America\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNquFJqwquvU7VE46ygDthM","collapsed_sections":[],"mount_file_id":"1pec713wbEk2VY90WvLJ1c2_lgS922QIL","provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"dca257c31c4f8f7084c1ff8fbab5f98247bf991475c7a63a8b70747f2fab47ca"}}},"nbformat":4,"nbformat_minor":0}
